{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Forecasting in Electiricity Markets: <br>Prediction Interval Averaging vs Quantile Regression Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- Christoffersen, P. F. (1998). Evaluating interval forecasts. International economic review, 841-862.\n",
    "- Gaba, A., Tsetlin, I., & Winkler, R. L. (2017). Combining interval forecasts. Decision Analysis, 14(1), 1-20.\n",
    "- Nowotarski, J., & Weron, R. (2018). Recent advances in electricity price forecasting: A review of probabilistic forecasting. Renewable and Sustainable Energy Reviews, 81, 1548-1568.\n",
    "- Uniejewski, B., Nowotarski, J., & Weron, R. (2016). Automated variable selection and shrinkage for day-ahead electricity price forecasting. Energies, 9(8), 621.\n",
    "- Weron, R., & Misiorek, A. (2008). Forecasting spot electricity prices: A comparison of parametric and semiparametric time series models. International journal of forecasting, 24(4), 744-763."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- from **2018-04-02** to 2019-03-31 individual model training\n",
    "- from **2019-04-01** to 2020-03-29 individual model forecasting & PI training\n",
    "- from **2020-03-30** to 2021-03-28 individual model forecasting & PI forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from code.holiday import get_holiday\n",
    "from code.forecast import get_forecast_AR, get_forecast_QRA, get_naive_forecast, PI_combinations\n",
    "from code.evaluation import get_PI_from_distribution, Christoffersen_scores, pinball_loss, winkler_score, DM_test\n",
    "from statsmodels.tools.sm_exceptions import ValueWarning\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# ignore the frequency error in ARIMA function\n",
    "warnings.filterwarnings('ignore', category=ValueWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df_raw = pd.read_csv('data/MCP-25032018-28032021.csv')\n",
    "load_df_raw = pd.read_csv('data/LoadForecast-01042018-28032021.csv')\n",
    "# after 03/10/2018 -> permanent +03 gmt\n",
    "\n",
    "preprocess = lambda df: df.set_index( \n",
    "    # merge date and time to get datetime and set index\n",
    "    pd.to_datetime(\n",
    "        df['Date'] + \" \" + df['Hour'],\n",
    "        dayfirst=True\n",
    "    )\n",
    ") \\\n",
    ".drop(columns=['Date', 'Hour']) \\\n",
    ".apply(\n",
    "    # remove decimal seperator \",\"\n",
    "    lambda col: col.astype(str).str.replace(',','').astype(float)\n",
    ") \\\n",
    ".tz_localize('Europe/Istanbul') # add timezone info\n",
    "\n",
    "price_df = preprocess(price_df_raw)\n",
    "price_df.rename(columns={x: re.findall('\\((.*)/', x)[0] for x in price_df.columns}, inplace=True)\n",
    "\n",
    "load_df = preprocess(load_df_raw).rename(columns={'Load Forecast (MWh)':'Load_Forecast_MWh'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to hourly shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly Dataframes\n",
    "## concatenate each hour's dataset vertically\n",
    "get_hourly_df = lambda df, column: df.groupby(df.index.hour)[column].apply(\n",
    "    lambda col: col.to_dict()).to_frame().dropna()\n",
    "\n",
    "hourly_load_df = get_hourly_df(load_df, 'Load_Forecast_MWh')\n",
    "hourly_price_df = pd.concat(\n",
    "    [ get_hourly_df(price_df, col) for col in price_df ],\n",
    "    axis=1\n",
    ")\n",
    "hourly_holiday_df = get_hourly_df(get_holiday(price_df), 'Holiday')\n",
    "hourly_min_price_l1 = pd.concat(\n",
    "    [get_hourly_df(price_df.resample('D').transform('min').shift(24), col) for col in price_df],\n",
    "    axis=1\n",
    ")\n",
    "hourly_min_price_l1.rename(columns={x: f\"{x}_min_l1\" for x in hourly_min_price_l1.columns}, inplace=True)\n",
    "\n",
    "hourly_weekday_df = hourly_price_df.assign(\n",
    "    Saturday = lambda df: (df.index.get_level_values(1).day_of_week == 5) * 1,\n",
    "    Sunday = lambda df: (df.index.get_level_values(1).day_of_week == 6) * 1,\n",
    "    Monday = lambda df: (df.index.get_level_values(1).day_of_week == 0) * 1,\n",
    ").filter(regex='day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose currency and merge the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "currency = 'USD' # 'MCP (TL/MWh), PTF (USD/MWh), PTF (EUR/MWh)\n",
    "main_df = pd.concat([\n",
    "    hourly_price_df[currency].to_frame(name='Price_MWh').groupby(level=0, axis=0).apply(lambda hour: hour.diff(1)),\n",
    "    hourly_load_df.groupby(level=0, axis=0).apply(lambda hour: hour.diff(1)),\n",
    "    hourly_min_price_l1.filter(regex=currency).rename(columns={f\"{currency}_min_l1\": \"Price_MWh_min_l1\"})\n",
    "    .groupby(level=0, axis=0).apply(lambda hour: hour.diff(1)),\n",
    "    hourly_holiday_df,\n",
    "    hourly_weekday_df\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "price_df_final = price_df[currency].to_frame(name='Price_MWh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_df_final.to_parquet('./data/Price_df_processed.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Models\n",
    "Three different models are employed; ARX (Autoregresive Model with exogenous variables), mARX (multi-day ARX) and TARX (Threshold Autoregressive model with exogenous variables). ARX and mARX models follows the the structure in Nowotarski & Weron(2018), and TARX model follows the structure explained in Weron & Misiorek (2008). Models with lags 1 to 7 are also tried to be regularized using _Lasso_ (least absolute shrinkage and selection operator), its implementation is inspired by Uniejewski et al. (2016).\n",
    "\n",
    "Each Model is constructed seperately for each hour due to correlative behavior of prices in a day. Except models with lasso, all models are estimated using statsmodels' ARIMA function with yule walker equations as estimator.\n",
    "\n",
    "\n",
    "### Notation:\n",
    "$p_{d,h}$: the at $h^{th}$ hour in day $d$.<br>\n",
    "$p_{d-1,h}^{min}$: minimum price in the previous day.<br>\n",
    "$load_{d,h}$: Load forecast for $h^{th}$ hour in day $d$.<br>\n",
    "$D_{day, h}$: Dummy for $day \\in\\{Saturday, Sunday, Monday, Holiday\\}$ where holiday is national holidays in Turkey.<br>\n",
    "**Note**: Due to stationarity concerns; price, price lags, minimum prices and load forecasts are differenced by 1.\n",
    "\n",
    "## Models\n",
    "\n",
    "### ARX\n",
    "**With lags 1,2,7**<br>\n",
    "$p_{d,h} = \\beta_{h,0} + \\beta_{h,1}p_{d-1,h} + \\beta_{h,2}p_{d-2,h} + \\beta_{h,3}p_{d-7,h} + \\beta_{h,4}p_{d-1}^{min} + \\beta_{h,5}load_{d,h} + \\beta_{h,6}D_{Sat,h} + \\beta_{h,7}D_{Sun,h} + \\beta_{h,8}D_{Mon,h} + \\beta_{h,9}D_{Holiday,h} + \\epsilon_{d,h}$ <br>\n",
    "\n",
    "**With lags 1,2,...,7**<br>\n",
    "$p_{d,h} = \\beta_{h,0} + \\sum_{i=0}^{7} \\beta_{h,i}p_{d-i,h} + \\beta_{h,8}p_{d-1}^{min} + \\beta_{h,9}load_{d,h} + \\beta_{h,10}D_{Sat,h} + \\beta_{h,11}D_{Sun,h} + \\beta_{h,12}D_{Mon,h} + \\beta_{h,13}D_{Holiday,h}+ \\epsilon_{d,h}$ <br>\n",
    "\n",
    "### TARX\n",
    "If mean of yesterday prices is higher than mean of 8 days ago than state 1 else state 0.<br>\n",
    "\n",
    "**With lags 1,2,7**<br>\n",
    "$p_{d,h} = \\beta_{h,0} +\\sum_{j \\in {0,1}}( \\beta_{h,1,j}p_{d-1,h} + \\beta_{h,2,j}p_{d-2,h} + \\beta_{h,3,j}p_{d-7,h} + \\beta_{h,4,j}p_{d-1}^{min} + \\beta_{h,5,j}load_{d,h} + \\beta_{h,6,j}D_{Sat,h} + \\beta_{h,7,j}D_{Sun,h} + \\beta_{h,8,j}D_{Mon,h} + \\beta_{h,9,j}D_{Holiday,h} )+ \\epsilon_{d,h}$ <br>\n",
    "for _j_ in {0,1} as state 0 and state 1.<br>\n",
    "\n",
    "**With lags 1,2,...,7**<br>\n",
    "$p_{d,h} = \\beta_{h,0} + \\sum_{j \\in {0,1}}( \\sum_{i=0}^{7} \\beta_{h,i,j}p_{d-l,h} + \\beta_{h,8,j}p_{d-1}^{min} + \\beta_{h,9,j}load_{d,h} + \\beta_{h,10,j}D_{Sat,h} + \\beta_{h,11,j}D_{Sun,h} + \\beta_{h,12,j}D_{Mon,h} + \\beta_{h,13,j}D_{Holiday,h} )+ \\epsilon_{d,h}$ <br>\n",
    "for _j_ in {0,1} as state 0 and state 1.<br>\n",
    "\n",
    "### mARX\n",
    "**With lags 1,2,7**<br>\n",
    "$p_{d,h} = \\beta_{h,0} +\\beta_{h,1}p_{d-1,h} + \\beta_{h,2}p_{d-2,h} + \\beta_{h,3}p_{d-7,h} + \\beta_{h,4}p_{d-1}^{min} + \\beta_{h,5}load_{d,h} + \\beta_{h,6}D_{Sat,h} + \\beta_{h,7}D_{Sun,h} + \\beta_{h,8}D_{Mon,h} + \\beta_{h,9}D_{Holiday,h} + \\beta_{h,6}D_{Sat,h}p_{d-1,h} + \\beta_{h,7}D_{Sun,h}p_{d-1,h} + \\beta_{h,8}D_{Mon,h}p_{d-1,h}+ \\epsilon_{d,h}$ <br>\n",
    "\n",
    "**With lags 1,2,...,7**<br>\n",
    "$p_{d,h} = \\beta_{h,0} + \\sum_{i=0}^{7} \\beta_{h,l}p_{d-i,h} + \\beta_{h,8}load_{d,h} + \\beta_{h,9}p_{d-1}^{min} + \\beta_{h,10}D_{Sat,h} + \\beta_{h,11}D_{Sun,h} + \\beta_{h,12}D_{Mon,h} + \\beta_{h,13}D_{Holiday,h} + \\beta_{h,14}D_{Sat,h}p_{d-1,h} + \\beta_{h,15}D_{Sun,h}p_{d-1,h} + \\beta_{h,16}D_{Mon,h}p_{d-1,h}+ \\epsilon_{d,h}$ <br>\n",
    "\n",
    "## Lasso ## ADD LARS\n",
    "For each model with lags 1,2,...,7, a regulariazition method called Lasso is employed and insignificant coefficients are forced to be zero in the model. Coefficients of lasso are calculated as such; <br>\n",
    "$\\beta^{Lasso} = \\underset{\\beta_{h,i}}{\\operatorname{argmin}} \\left\\{ \\sum_{d,h} \\left( p_{d,h} - \\sum_{i}^{n} \\beta_{h, i}X_{h,i} \\right)^{2} + \\lambda \\sum_{i}^{n} |\\beta_{h,i}| \\right\\}$ <br>\n",
    "where $\\lambda$ is chosen using point forecasts between period 2019-04-01 and 2020-03-29."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasts\n",
    "Point forecasts for each hour are acquired using the last year's values (52\\*7 days) as training and one-step ahead forecast is calculated by also taking into account of exogenous variables (load, min price and such.)\n",
    "\n",
    "\n",
    "Distributional forecasts are calculated in three ways;\n",
    "- Historical; residuals of the model, i.e. the prediction error in the last year.\n",
    "- Distributional; random values from a zero-mean normal distibution with std of residuals.\n",
    "- Bootstrap; a new price series is estimated using the coefficients of the model and a value is forecasted; for 250 times. This process provides a distribution of forecasts (with population of 250).\n",
    "\n",
    "**Note**: Since the first differenced series are used in the models, the point and distributional forecasts are the differences from the last value. In the forecasting function, the previous value added to point forecast to get the adjusted forecast which is added to distributional forecast to skew the distribution and pile the distribution around the adjusted point forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'ARX_1_2_7','ARX_to_7', 'ARX_to_7_w_lasso_aic',\n",
    "    'TARX_1_2_7','TARX_to_7', 'TARX_to_7_w_lasso_aic',\n",
    "    'mARX_1_2_7','mARX_to_7', 'mARX_to_7_w_lasso_aic',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_point = ('2019-04-01', '2020-03-29')\n",
    "p_path = './data/Point_forecasts'\n",
    "for model in model_names:\n",
    "    lags = [1,2,7] if '1_2_7' in model else range(1,8)\n",
    "    model_type = re.match('.*(AR)', model)[0]\n",
    "    method = 'lasso' if 'lasso' in model else 'ols'\n",
    "    # construct the model\n",
    "    globals()[model] = get_forecast_AR(\n",
    "        main_df,\n",
    "        price_df_final,\n",
    "        lags,\n",
    "        forecast_dates=dates_point,\n",
    "        model_type=model_type,\n",
    "        method=method,\n",
    "        lasso_args={'criterion': 'aic', 'fit_intercept': True, 'normalize': False},\n",
    "    )\n",
    "    globals()[model].to_parquet(f'{p_path}/{model}_Point_forecast.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributional Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_distributional = ('2020-03-30', '2021-03-28')\n",
    "d_path = './data/Distributional_forecasts'\n",
    "for model in model_names:\n",
    "    # adjust parameters\n",
    "    lags = [1,2,7] if '1_2_7' in model else range(1,8)\n",
    "    model_type = re.match('.*(AR)', model)[0]\n",
    "    method = 'lasso' if 'lasso' in model else 'ols'\n",
    "    # construct the model\n",
    "    globals()[model + '_dist'] = get_forecast_AR(\n",
    "        main_df,\n",
    "        price_df_final,\n",
    "        lags,\n",
    "        forecast_dates=dates,\n",
    "        model_type=model_type,\n",
    "        method=method,\n",
    "        lasso_args={'criterion': 'aic', 'fit_intercept': True, 'normalize': False},\n",
    "        PI_calculate=True,\n",
    "        bootstrap_B=250,\n",
    "    )\n",
    "    globals()[model + '_dist'].to_parquet(f'{d_path}/{file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Regression Averaging\n",
    "Construct distribution of forecasts using point forecasts of individual models and quantile regression model (Nowotarski & Weron, 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct QRA\n",
    "qra_range = np.array([0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "qra_df = pd.concat([\n",
    "    ARX_1_2_7.Forecast_added.to_frame(name='ARX_1_2_7'),\n",
    "    ARX_to_7.Forecast_added.to_frame(name='ARX_to_7'),\n",
    "    ARX_to_7_w_lasso_aic.Forecast_added.to_frame(name='ARX_to_7_w_lasso'),\n",
    "    TARX_1_2_7.Forecast_added.to_frame(name='TARX_1_2_7'),\n",
    "    TARX_to_7.Forecast_added.to_frame(name='TARX_to_7'),\n",
    "    TARX_to_7_w_lasso_aic.Forecast_added.to_frame(name='TARX_to_7_w_lasso'),\n",
    "    mARX_1_2_7.Forecast_added.to_frame(name='mARX_1_2_7'),\n",
    "    mARX_to_7.Forecast_added.to_frame(name='mARX_to_7'),\n",
    "    mARX_to_7_w_lasso_aic.Forecast_added.to_frame(name='mARX_to_7_w_lasso'),\n",
    "    price_df_final.loc[ARX_1_2_7.index]\n",
    "], axis=1)\n",
    "\n",
    "qra_forecast_df = get_forecast_QRA(qra_df, qra_range, dates_distributional)\n",
    "qra_forecast_df.to_parquet('./data/QRA_forecasts/qra_forecast_df_general.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct QRA for models 1,2,7\n",
    "qra_df_1_2_7 = pd.concat([\n",
    "    ARX_1_2_7.Forecast_added.to_frame(name='ARX_1_2_7'),\n",
    "    TARX_1_2_7.Forecast_added.to_frame(name='TARX_1_2_7'),\n",
    "    mARX_1_2_7.Forecast_added.to_frame(name='mARX_1_2_7'),\n",
    "    price_df_final.loc[ARX_1_2_7.index]\n",
    "], axis=1)\n",
    "\n",
    "qra_forecast_df_1_2_7 = get_forecast_QRA(qra_df_1_2_7, qra_range, dates_distributional)\n",
    "qra_forecast_df_1_2_7.to_parquet('./data/QRA_forecasts/qra_forecast_df_1_2_7.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct QRA for models to 7\n",
    "qra_df_to_7 = pd.concat([\n",
    "    ARX_to_7.Forecast_added.to_frame(name='ARX_to_7'),\n",
    "    TARX_to_7.Forecast_added.to_frame(name='TARX_to_7'),\n",
    "    mARX_to_7.Forecast_added.to_frame(name='mARX_to_7'),\n",
    "    price_df_final.loc[ARX_1_2_7.index]\n",
    "], axis=1)\n",
    "\n",
    "qra_forecast_df_to_7 = get_forecast_QRA(qra_df_to_7, qra_range, dates_distributional)\n",
    "qra_forecast_df_to_7.to_parquet('./data/QRA_forecasts/qra_forecast_df_to_7.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct QRA models to 7 with lasso\n",
    "qra_df_to_7_w_lasso = pd.concat([\n",
    "    ARX_to_7_w_lasso_aic.Forecast_added.to_frame(name='ARX_to_7_w_lasso'),\n",
    "    TARX_to_7_w_lasso_aic.Forecast_added.to_frame(name='TARX_to_7_w_lasso'),\n",
    "    mARX_to_7_w_lasso_aic.Forecast_added.to_frame(name='mARX_to_7_w_lasso'),\n",
    "    price_df_final.loc[ARX_1_2_7.index]\n",
    "], axis=1)\n",
    "\n",
    "qra_forecast_df_to_7_w_lasso = get_forecast_QRA(qra_df_to_7_w_lasso, qra_range, dates_distributional)\n",
    "qra_forecast_df_to_7_w_lasso.to_parquet('./data/QRA_forecasts/qra_forecast_df_to_7_w_lasso.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Forecast\n",
    "Each hour in Tuesday, Wednesday, Thursday and Friday equals to previous day's value, each hour in Monday, Saturday and Sunday is the same as last week's value (Nowotarski & Weron, 2018). Distributional forecasts are calculated using the error of point forecast in the last year (subsequent 52*7 days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAIVE forecast for total data\n",
    "naive_forecast_df, auto_ar_forecast_df = get_naive_forecast(price_df_final, forecast_dates=('2020-03-30', '2021-03-28'))\n",
    "naive_forecast_df.to_parquet('./data/Benchmark_forecasts/Naive_forecast_df.parquet')\n",
    "auto_ar_forecast_df.to_parquet('./data/Benchmark_forecasts/AutoArima_forecast_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_df = pd.concat([\n",
    "#     qra_df.loc[sample_week[0]:],\n",
    "#     qra_forecast_df['0.05'].to_frame('QRA Lower'),\n",
    "#     qra_forecast_df['0.5'].to_frame('QRA Median'),\n",
    "#     qra_forecast_df['0.95'].to_frame('QRA Upper'),\n",
    "#     naive_forecast_df.loc[sample_week[0]:sample_week[1]].Naive_forecast\n",
    "# ],axis=1\n",
    "# )\n",
    "\n",
    "# fig = go.Figure()\n",
    "# for col in fig_df:\n",
    "#     fig.add_trace(go.Scatter(y=fig_df[col], x=fig_df.index, name=col))\n",
    "# fig.update_layout(\n",
    "#     hovermode=\"x\",\n",
    "#     title=f'Sample Plot for {sample_week[0]} to {sample_week[1]}'\n",
    "# )\n",
    "# fig.show('png')\n",
    "# # fig.show() to get the interactive plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Prediction Intervals and combinations\n",
    "This section grabs the distributional forecast of each model, combines them by utilizing various methods to get a lower and a upper bound following Gaba et al. (2017);\n",
    "- Mean; simple average of lower and upper bounds of each model included.\n",
    "- Median; median value of lower and upper bounds of each model included.\n",
    "- Envelope; minimum of lower bounds and maximum of upper bounds.\n",
    "- Interior trimming; excludes upper $\\beta$ percent of the lower bounds and lower $\\beta$ percent of upper bounds and takes a simple average. (Wider than simple average given $1>\\beta>0$)\n",
    "- Exterior trimming; excludes lower $\\beta$ percent of the lower bounds and upper $\\beta$ percent of upper bounds and takes a simple average. (Narrower than simple average given $1>\\beta>0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_names:\n",
    "    globals()[\"PI_\"+model] = get_PI_percentiles(eval(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate simple combinations\n",
    "PI_simple_combinations_df = PI_combinations(\n",
    "    models=[\n",
    "        PI_ARX_1_2_7.PI_historical,\n",
    "        PI_ARX_1_2_7.PI_distributional,\n",
    "        PI_ARX_1_2_7.PI_bootstrap,\n",
    "        PI_ARX_to_7.PI_historical,\n",
    "        PI_ARX_to_7.PI_distributional,\n",
    "        PI_ARX_to_7.PI_bootstrap,\n",
    "        PI_ARX_to_7_w_lasso_aic.PI_historical,\n",
    "        PI_ARX_to_7_w_lasso_aic.PI_distributional,\n",
    "        PI_ARX_to_7_w_lasso_aic.PI_bootstrap,\n",
    "        PI_TARX_1_2_7.PI_historical,\n",
    "        PI_TARX_1_2_7.PI_distributional,\n",
    "        PI_TARX_to_7.PI_historical,\n",
    "        PI_TARX_to_7.PI_distributional,\n",
    "        PI_TARX_to_7_w_lasso_aic.PI_historical,\n",
    "        PI_TARX_to_7_w_lasso_aic.PI_distributional,\n",
    "        PI_mARX_1_2_7.PI_historical,\n",
    "        PI_mARX_1_2_7.PI_distributional,\n",
    "        PI_mARX_1_2_7.PI_bootstrap,\n",
    "        PI_mARX_to_7.PI_historical,\n",
    "        PI_mARX_to_7.PI_distributional,\n",
    "        PI_mARX_to_7.PI_bootstrap,\n",
    "        PI_mARX_to_7_w_lasso_aic.PI_historical,\n",
    "        PI_mARX_to_7_w_lasso_aic.PI_distributional,\n",
    "        PI_mARX_to_7_w_lasso_aic.PI_bootstrap,\n",
    "    ],\n",
    "    PI = [0.5, 0.9],\n",
    "    beta = 0.4 # for trimming\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IC based combination\n",
    "weights_dict = {x: eval(x).model_aic for x in model_names}\n",
    "weights_df = pd.concat(weights_dict.values(), axis=1, keys=weights_dict.keys()) \\\n",
    ".dropna() \\\n",
    ".apply(lambda x: x.apply(lambda y: y - min(x)), axis=1) \\\n",
    ".apply(lambda x: x.apply(lambda y: np.exp((-1/2)*y) / sum(np.exp((-1/2)*x))), axis=1)\n",
    "\n",
    "get_IC_based_combination = lambda PI_type: pd.concat(\n",
    "    [eval('PI_'+ x)[PI_type] for x in model_names],\n",
    "    axis=1,\n",
    "    keys=model_names\n",
    ") \\\n",
    ".groupby(axis=1, level=1) \\\n",
    ".apply(lambda g: g.dropna().multiply(weights_df.values).sum(axis=1))\n",
    "\n",
    "PI_IC_combinations_historical = get_IC_based_combination('PI_historical')\n",
    "PI_IC_combinations_distributional = get_IC_based_combination('PI_distributional')\n",
    "PI_IC_combinations_bootstrap = get_IC_based_combination('PI_bootstrap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reliability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get percentiles of naive distributional forecast\n",
    "Since the negative prices are not allowed in Turkish market, negative values are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_df_naive = get_PI_from_distribution(\n",
    "    naive_forecast_df.PI_historical,\n",
    "    return_negative_values=False,\n",
    "    percentiles=np.array([5, 25, 50, 75, 95])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unconditional and Conditional Coverage\n",
    "#### Showcase using naive forecast\n",
    "Unconditional coverage likelihood ratio (i.e. Kupiec's proportion of failures) test and Conditional Coverage likelihood ratio test (Nowotarski & Weron, 2018; Christoffersen, 1998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [1,2,7]# [1,2,7]\n",
    "def plot_coverage(percentile_df, price_df, lags, title):\n",
    "    \n",
    "    Coverage_LR_Scores_50 = Christoffersen_scores(\n",
    "        percentile_df=percentile_df.dropna(),\n",
    "        realized=price_df,\n",
    "        PI=0.5,\n",
    "        lags=lags\n",
    "    )\n",
    "    Coverage_LR_Scores_90 = Christoffersen_scores(\n",
    "        percentile_df=percentile_df,\n",
    "        realized=price_df,\n",
    "        PI=0.9,\n",
    "        lags=lags\n",
    "    )\n",
    "\n",
    "    fig, ax= plt.subplots(len(lags)+1, figsize=(10,10))\n",
    "    ax[0].set_title(f'Unconditional Coverage - {title}')\n",
    "    Coverage_LR_Scores_90.UC_LR.apply(lambda x: 20 if x>20 else x).reset_index().plot(\n",
    "        x='index', y='UC_LR', kind='scatter', ax=ax[0], color='blue', label='90% PI'\n",
    "    )\n",
    "    Coverage_LR_Scores_50.UC_LR.apply(lambda x: 20 if x>20 else x).reset_index().plot(\n",
    "        x='index', y='UC_LR', kind='scatter', ax=ax[0], color='red', label='50% PI', xlabel='hour'\n",
    "    )\n",
    "    ax[0].axhline(y=stats.chi2.ppf(0.99,df=1))\n",
    "    ax[0].axhline(y=stats.chi2.ppf(0.95,df=1), ls='--')\n",
    "\n",
    "    for i, l in zip(range(1,len(lags)+1), lags):\n",
    "        ax[i].set_title(f'Conditional Coverage - {title} ({l} day lag)')\n",
    "        Coverage_LR_Scores_90[f'CC_LR_lag{l}'].apply(lambda x: 20 if x>20 else x).reset_index().plot(\n",
    "            x='index', y=f'CC_LR_lag{l}', kind='scatter', ax=ax[i], color='blue', label='90% PI'\n",
    "        )\n",
    "        Coverage_LR_Scores_50[f'CC_LR_lag{l}'].apply(lambda x: 20 if x>20 else x).reset_index().plot(\n",
    "            x='index', y=f'CC_LR_lag{l}', kind='scatter', ax=ax[i], color='red', label='50% PI',  xlabel='hour'\n",
    "        )\n",
    "        ax[i].axhline(y=stats.chi2.ppf(0.99,df=2))\n",
    "        ax[i].axhline(y=stats.chi2.ppf(0.95,df=2), ls='--')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_coverage(PI_ARX_1_2_7.PI_historical, price_df_final, lags=[1,2,7], title='ARX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winkler Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Naive\n",
    "winkler_naive_90 = winkler_score(percentile_df_naive, price_df_final, PI=0.9)\n",
    "winkler_naive_50 = winkler_score(percentile_df_naive, price_df_final, PI=0.5)\n",
    "### ARX 1,2,7\n",
    "winkler_ARX127_90 = winkler_score(get_PI_wrapper_csv(ARX_1_2_7.dropna()).PI_bootstrap, price_df_final, PI=0.9)\n",
    "winkler_ARX127_50 = winkler_score(get_PI_wrapper_csv(ARX_1_2_7.dropna()).PI_bootstrap, price_df_final, PI=0.5)\n",
    "# QRA\n",
    "winkler_qra_90 = winkler_score(qra_forecast_df, price_df_final, PI=0.9)\n",
    "winkler_qra_50 = winkler_score(qra_forecast_df, price_df_final, PI=0.5)\n",
    "### PI combinations\n",
    "winkler_PIcombinations_90_df = PI_combinations_df.groupby(level=0, axis=1).apply(\n",
    "    lambda method: winkler_score(method.droplevel(0,1), price_df_final, PI=0.9)\n",
    ")\n",
    "winkler_PIcombinations_50_df = PI_combinations_df.groupby(level=0, axis=1).apply(\n",
    "    lambda method: winkler_score(method.droplevel(0,axis=1), price_df_final, PI=0.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diebold - Mariano test\n",
    "Test whether the difference in winkler scores is significant for two competing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% PI\n",
    "DM_winkler_ARX127_vs_naive_90 = DM_test(winkler_ARX127_90.Winkler_score, winkler_naive_90.Winkler_score, alpha=0.1, two_sided=True)\n",
    "DM_winkler_PIcomb_vs_ARX127_90 = winkler_PIcombinations_90_df.groupby(level=0, axis=1).apply(\n",
    "    lambda method: DM_test(method.droplevel(0,1).Winkler_score, winkler_ARX127_90.Winkler_score, alpha=0.1, two_sided=True)\n",
    ")\n",
    "DM_winkler_PIcomb_vs_naive_90 = winkler_PIcombinations_90_df.groupby(level=0, axis=1).apply(\n",
    "    lambda method: DM_test(method.droplevel(0,1).Winkler_score, winkler_naive_90.Winkler_score, alpha=0.1, two_sided=True)\n",
    ")\n",
    "DM_winkler_PIcomb_vs_qra_90 = winkler_PIcombinations_90_df.groupby(level=0, axis=1).apply(\n",
    "    lambda method: DM_test(method.droplevel(0,1).Winkler_score, winkler_qra_90.Winkler_score, alpha=0.1, two_sided=True)\n",
    ")\n",
    "# 50% PI\n",
    "DM_winkler_ARX127_vs_naive_50 = DM_test(winkler_ARX127_50.Winkler_score, winkler_naive_50.Winkler_score, alpha=0.1, two_sided=True)\n",
    "DM_winkler_PIcomb_vs_ARX127_50 = winkler_PIcombinations_50_df.groupby(level=0, axis=1).apply(\n",
    "    lambda method: DM_test(method.droplevel(0,1).Winkler_score, winkler_ARX127_50.Winkler_score, alpha=0.1, two_sided=True)\n",
    ")\n",
    "DM_winkler_PIcomb_vs_naive_50 = winkler_PIcombinations_50_df.groupby(level=0, axis=1).apply(\n",
    "    lambda method: DM_test(method.droplevel(0,1).Winkler_score, winkler_naive_50.Winkler_score, alpha=0.1, two_sided=True)\n",
    ")\n",
    "DM_winkler_PIcomb_vs_qra_50 = winkler_PIcombinations_50_df.groupby(level=0, axis=1).apply(\n",
    "    lambda method: DM_test(method.droplevel(0,1).Winkler_score, winkler_qra_50.Winkler_score, alpha=0.1, two_sided=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_winkler_PIcomb_vs_naive_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
